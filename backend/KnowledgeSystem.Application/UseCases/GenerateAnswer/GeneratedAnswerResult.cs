using KnowledgeSystem.Application.UseCases.Prompting;
using KnowledgeSystem.Domain.ValueObjects;

namespace KnowledgeSystem.Application.UseCases.GenerateAnswer;

/// <summary>
/// Result of LLM-based answer generation.
/// Contains the generated answer, source references, and confidence assessment.
/// </summary>
public sealed class GeneratedAnswerResult
{
    /// <summary>
    /// The answer generated by the LLM based on retrieved document chunks.
    /// </summary>
    public required string Answer { get; init; }

    /// <summary>
    /// Source chunks that were used as context for generating the answer.
    /// Used for citation, traceability, and UI display.
    /// </summary>
    public required IReadOnlyCollection<PromptSourceChunk> Sources { get; init; }

    /// <summary>
    /// Confidence level of the answer based on semantic similarity.
    /// - High: Strong match, reliable answer
    /// - Low: Partial match, uncertain answer
    /// - None: No relevant information found
    /// </summary>
    public required ConfidenceLevel ConfidenceLevel { get; init; }

    /// <summary>
    /// Human-readable explanation of the confidence level.
    /// Example: "High confidence - strong match with documents (similarity: 87.3%)"
    /// </summary>
    public required string ConfidenceExplanation { get; init; }

    /// <summary>
    /// The original user query (for reference and logging).
    /// </summary>
    public required string OriginalQuery { get; init; }

    /// <summary>
    /// Number of source chunks used to generate the answer.
    /// </summary>
    public int SourceCount => Sources.Count;

    /// <summary>
    /// Whether the LLM was actually invoked to generate the answer.
    /// False if confidence was too low or no chunks were retrieved.
    /// </summary>
    public required bool LlmInvoked { get; init; }

    /// <summary>
    /// Optional: detected language of the query/answer.
    /// </summary>
    public string? Language { get; init; }
}

